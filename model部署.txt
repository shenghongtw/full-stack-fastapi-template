pip install git+https://github.com/huggingface/transformers@f3f6c86582611976e72be054675e2bf0abb5f775 accelerate qwen-vl-utils 'vllm>=0.7.2'

curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash
apt install -y git-lfs
git lfs install

git clone https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct
mv Qwen2.5-VL-7B-Instruct Xiao-7B-Instruct

vllm serve Xiao-7B-Instruct --max-model-len 4096 --max-num-seqs 8 --port 8000 --host 0.0.0.0 --dtype bfloat16 --limit-mm-per-prompt image=5,video=5
#vllm serve Qwen/Qwen2.5-VL-7B-Instruct --max-model-len 4096 --max-num-seqs 8 --port 8000 --host 0.0.0.0 --dtype bfloat16 --limit-mm-per-prompt image=5,video=5

＃呼叫vllm的api
import base64
from openai import OpenAI
# Set OpenAI's API key and API base to use vLLM's API server.
openai_api_key = "EMPTY"
openai_api_base = "http://localhost:8000/v1"
client = OpenAI(
    api_key=openai_api_key,
    base_url=openai_api_base,
)
image_path = "8d1a3bdc-5984-4603-ae06-b6cddcc2114e.jpg"
with open(image_path, "rb") as f:
    encoded_image = base64.b64encode(f.read())
encoded_image_text = encoded_image.decode("utf-8")
base64_qwen = f"data:image;base64,{encoded_image_text}"
chat_response = client.chat.completions.create(
    model="Xiao-7B-Instruct",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {
            "role": "user",
            "content": [
                {
                    "type": "image_url",
                    "image_url": {
                        "url": base64_qwen
                    },
                },
                {"type": "text", "text": "以json格式列出圖片中的姓名、地址、電子郵件(email)、電話、公司名稱、職位名稱,沒有資料的話則回傳none,如果有中文優先回傳中文"},
            ],
        },
    ],
)
print("Chat response:", chat_response)